{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "07/02/2020 13:28:44 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/02/2020 13:28:44 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "07/02/2020 13:28:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output/quote', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul02_13-28-44_localhost.localdomain', logging_first_step=False, logging_steps=20, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "07/02/2020 13:28:50 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/qwelian/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/02/2020 13:28:50 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/02/2020 13:28:50 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/qwelian/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/02/2020 13:28:50 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/02/2020 13:28:51 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/qwelian/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "07/02/2020 13:28:51 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/qwelian/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "/home/qwelian/.local/share/virtualenvs/nlp_api-galoEe92/lib/python3.8/site-packages/transformers/modeling_auto.py:789: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "07/02/2020 13:28:51 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /home/qwelian/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "07/02/2020 13:28:58 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "07/02/2020 13:28:58 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "07/02/2020 13:28:58 - INFO - filelock -   Lock 140630188867008 acquired on cached_lm_GPT2Tokenizer_1024_quote_train.txt.lock\n",
      "07/02/2020 13:28:58 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at \n",
      "07/02/2020 13:28:58 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file cached_lm_GPT2Tokenizer_1024_quote_train.txt [took 0.001 s]\n",
      "07/02/2020 13:28:58 - INFO - filelock -   Lock 140630188867008 released on cached_lm_GPT2Tokenizer_1024_quote_train.txt.lock\n",
      "07/02/2020 13:28:58 - INFO - filelock -   Lock 140630188866960 acquired on cached_lm_GPT2Tokenizer_1024_quote_valid.txt.lock\n",
      "07/02/2020 13:28:58 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at \n",
      "07/02/2020 13:28:58 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file cached_lm_GPT2Tokenizer_1024_quote_valid.txt [took 0.000 s]\n",
      "07/02/2020 13:28:58 - INFO - filelock -   Lock 140630188866960 released on cached_lm_GPT2Tokenizer_1024_quote_valid.txt.lock\n",
      "07/02/2020 13:28:58 - INFO - transformers.trainer -   Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: psutil not installed, only GPU stats will be reported.  Install with pip install psutil\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200702_172858-pvsv54ck\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33micy-wildflower-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/qwelian/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/qwelian/huggingface/runs/pvsv54ck\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "07/02/2020 13:28:59 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "07/02/2020 13:28:59 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -     Num examples = 2\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -     Num Epochs = 4\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/02/2020 13:28:59 - INFO - transformers.trainer -     Total optimization steps = 8\n",
      "Epoch:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1/2 [00:07<00:07,  7.87s/it]\u001b[A\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.76s/it]\u001b[A\n",
      "Epoch:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1/4 [00:15<00:46, 15.52s/it]\n",
      "Iteration:   0%|                                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1/2 [00:07<00:07,  7.80s/it]\u001b[A\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.91s/it]\u001b[A\n",
      "Epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2/4 [00:31<00:31, 15.61s/it]\n",
      "Iteration:   0%|                                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1/2 [00:08<00:08,  8.23s/it]\u001b[A\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.32s/it]\u001b[A\n",
      "Epoch:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3/4 [00:47<00:15, 15.92s/it]\n",
      "Iteration:   0%|                                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1/2 [00:08<00:08,  8.50s/it]\u001b[A\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.46s/it]\u001b[A\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:04<00:00, 16.23s/it]\n",
      "07/02/2020 13:30:04 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/02/2020 13:30:04 - INFO - transformers.trainer -   Saving model checkpoint to output/quote\n",
      "07/02/2020 13:30:04 - INFO - transformers.configuration_utils -   Configuration saved in output/quote/config.json\n",
      "07/02/2020 13:30:04 - INFO - transformers.modeling_utils -   Model weights saved in output/quote/pytorch_model.bin\n",
      "07/02/2020 13:30:04 - INFO - __main__ -   *** Evaluate ***\n",
      "07/02/2020 13:30:04 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/02/2020 13:30:04 - INFO - transformers.trainer -     Num examples = 0\n",
      "07/02/2020 13:30:04 - INFO - transformers.trainer -     Batch size = 8\n",
      "Evaluation: 0it [00:00, ?it/s]\n",
      "07/02/2020 13:30:04 - INFO - transformers.trainer -   {'epoch': 4.0, 'step': 8}\n",
      "Traceback (most recent call last):\n",
      "  File \"run_language_modeling.py\", line 281, in <module>\n",
      "    main()\n",
      "  File \"run_language_modeling.py\", line 259, in main\n",
      "    perplexity = math.exp(eval_output[\"eval_loss\"])\n",
      "KeyError: 'eval_loss'\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 230661\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1. Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch 4.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp 1593711004.8908944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime 80.23651313781738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _step 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced icy-wildflower-4: https://app.wandb.ai/qwelian/huggingface/runs/pvsv54ck\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to download from Huggingface, create training data, and fine tune model\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Huggingface scripts for fine-tuning models and language generation\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py -q\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-generation/run_generation.py -q\n",
    "\n",
    "path = \"quotes.csv\"\n",
    "df = pd.read_csv(path)\n",
    "quote = \"quote\"\n",
    "\n",
    "# shuffle data\n",
    "random.shuffle(list(df.quote))\n",
    "\n",
    "# fraction of training data\n",
    "split_train_valid = 0.9\n",
    "\n",
    "# split dataset\n",
    "train_size = int(split_train_valid * len(list(df.quote)))\n",
    "valid_size = len(list(df.quote)) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(list(df.quote), [train_size, valid_size])\n",
    "\n",
    "\n",
    "with open('{}_train.txt'.format('quote'), 'w') as f:\n",
    "    f.write('\\n'.join(train_dataset))\n",
    "\n",
    "with open('{}_valid.txt'.format('quote'), 'w') as f:\n",
    "    f.write('\\n'.join(valid_dataset))\n",
    "\n",
    "!python3 run_language_modeling.py \\\n",
    "    --output_dir=output/$quote \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=$quote\\_train.txt \\\n",
    "    --do_eval --eval_data_file=$quote\\_valid.txt \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem: I neeed more data to fine tune each model to the persons speech pattern. \n",
    "    \n",
    "    I will basically be creating NLP philosphy generator\n",
    "    \n",
    "    this will result in 24 fine tuned models that represent each person, similar to the twitter models- calculate space cost of models, calculate time to train\n",
    "    \n",
    "    there is a trade off present: if I do not care about space I can just create and save a bunch of differnet models\n",
    "                                  if i do care, I will, train, test, create conversation, and upload in one go\n",
    "    \n",
    "    the first option seems straightforward, but the time to train the models could be problamatic, \n",
    "        and the space issue means that i am pretty much dealing with a local script that i cannot deploy somewhere.\n",
    "        i would eventually like to deploy this to a server and merely have a docker container.is this airflow use case?\n",
    "    \n",
    "    the second option could be faster, but i need to train one model for the 1st speaker and for each subsequent person\n",
    "    a marriage between the two will use the straightforward double for loop and check for model for diff users. if it doesnt exist, \n",
    "        train on the data for person, run algorithm. \n",
    "    \n",
    "## Problem: I still need an algorithm to generate a dialog between speaker x and speaker y\n",
    "    this may incorporate different pipelines\n",
    "        ex: prompt -> generate text -> summarize?, NER?, question answer? -> prompt\n",
    "    \n",
    "    a harder question is how to model the sequence format such that it resembles dialog\n",
    "    \n",
    "    I can just generate summerize the first speakers quotes, generate text, summerize, use as prompt for 2nd speaker, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
