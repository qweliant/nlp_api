{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at microsoft/DialoGPT-medium and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to download from Huggingface, create training data, and fine tune model\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import wandb\n",
    "from pymongo import MongoClient\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AdamW\n",
    ")\n",
    "wandb.login()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "MONGODB_NAME = os.environ['MONGODB_NAME']\n",
    "MONGODB_KEY = os.environ['MONGODB_KEY']\n",
    "MONGODB_DB = os.environ['MONGODB_DB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open client to mongodb      # Replace in the future woth MONGODB_CLIENT env variable\n",
    "client = MongoClient(f\"mongodb+srv://{MONGODB_NAME}:{MONGODB_KEY}@cluster0.7kkil.azure.mongodb.net/{MONGODB_DB}?retryWrites=true&w=majority\")\n",
    "db = client[f\"{MONGODB_DB}\"]\n",
    "personas = db.personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of text\n",
    "text = []\n",
    "for post in personas.find():\n",
    "    text.append(str(post['parsed_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(text)\n",
    "\n",
    "# fraction of training data\n",
    "split_train_valid = 0.9\n",
    "\n",
    "\n",
    "\n",
    "# split dataset\n",
    "train_size = int(split_train_valid * len(text))\n",
    "valid_size = len(text) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(text, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/qwelian/huggingface\" target=\"_blank\">https://app.wandb.ai/qwelian/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/qwelian/huggingface/runs/2y9nt321\" target=\"_blank\">https://app.wandb.ai/qwelian/huggingface/runs/2y9nt321</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./model/results',          # output directory\n",
    "#     num_train_epochs=3,              # total # of training epochs\n",
    "#     per_device_train_batch_size=16,  # batch size per device during training\n",
    "#     per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./model/logs',            # directory for storing logs\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "#     args=training_args,                  # training arguments, defined above\n",
    "#     train_dataset=train_dataset,         # training dataset\n",
    "#     eval_dataset=test_dataset            # evaluation dataset\n",
    "# )\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "from torch.nn import functional as F\n",
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "loss = F.cross_entropy(labels, outputs[0])\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem: I neeed more data to fine tune each model to the persons speech pattern. \n",
    "    \n",
    "    I will basically be creating NLP philosphy generator\n",
    "    \n",
    "    this will result in 24 fine tuned models that represent each person, similar to the twitter models- calculate space cost of models, calculate time to train\n",
    "    \n",
    "    there is a trade off present: if I do not care about space I can just create and save a bunch of differnet models\n",
    "                                  if i do care, I will, train, test, create conversation, and upload in one go\n",
    "    \n",
    "    the first option seems straightforward, but the time to train the models could be problamatic, \n",
    "        and the space issue means that i am pretty much dealing with a local script that i cannot deploy somewhere.\n",
    "        i would eventually like to deploy this to a server and merely have a docker container.is this airflow use case?\n",
    "    \n",
    "    the second option could be faster, but i need to train one model for the 1st speaker and for each subsequent person\n",
    "    a marriage between the two will use the straightforward double for loop and check for model for diff users. if it doesnt exist, \n",
    "        train on the data for person, run algorithm. \n",
    "    \n",
    "## Problem: I still need an algorithm to generate a dialog between speaker x and speaker y\n",
    "    this may incorporate different pipelines\n",
    "        ex: prompt -> generate text -> summarize?, NER?, question answer? -> prompt\n",
    "    \n",
    "    a harder question is how to model the sequence format such that it resembles dialog\n",
    "    \n",
    "    I can just generate summerize the first speakers quotes, generate text, summerize, use as prompt for 2nd speaker, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
